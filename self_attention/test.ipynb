{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "from python.SelfAttention import SelfAttention as SelfAttentionPython\n",
    "from extended.SelfAttention import SelfAttention as SelfAttentionExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class ExperimentConfig:\n",
    "    def __init__(self, device_pref = 'cuda', n_runs = 10000):\n",
    "        self.device = torch.device(device_pref if torch.cuda.is_available() else 'cpu')\n",
    "        self.n_runs = n_runs\n",
    "        \n",
    "    def set_attention_params(self, *args):\n",
    "        if len(args) == 1:\n",
    "            batch_size, sequence_length, heads, embed_size = args[0]\n",
    "        elif len(args) == 4:\n",
    "            batch_size, sequence_length, heads, embed_size = args\n",
    "        else:\n",
    "            raise Exception('invalid args format')\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.heads = heads\n",
    "        self.embed_size = embed_size\n",
    "        return self\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return self.get_config_log()\n",
    "    def get_config_log(self):\n",
    "        return f'Batch Size: {self.batch_size}, Seq Length: {self.sequence_length}, Heads: {self.heads}, Embed Size: {self.embed_size}'\n",
    "        \n",
    "def run_experiment(exp, ModelClass):\n",
    "    device = exp.device\n",
    "    n_runs = exp.n_runs\n",
    "    embed_size = exp.embed_size\n",
    "    heads = exp.heads\n",
    "    sequence_length = exp.sequence_length\n",
    "    batch_size = exp.batch_size\n",
    "    \n",
    "    values = torch.randn(batch_size, sequence_length, embed_size, device=device)\n",
    "    keys = torch.randn(batch_size, sequence_length, embed_size, device=device)\n",
    "    queries = torch.randn(batch_size, sequence_length, embed_size, device=device)\n",
    "    mask = torch.ones(1,sequence_length, device=device)  # No masking for simplicity\n",
    "\n",
    "    model = ModelClass(embed_size, heads)\n",
    "    model.reset_parameters()\n",
    "    model = model.to(device)\n",
    "\n",
    "    forward = 0\n",
    "    backward = 0\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        output = model(values, keys, queries, mask)\n",
    "        forward += time.time() - start\n",
    "\n",
    "        loss = output.sum() # only for perf testing\n",
    "        start = time.time()\n",
    "        loss.backward()\n",
    "        backward += time.time() - start\n",
    "    forward = forward*1e6/n_runs\n",
    "    backward = backward*1e6/n_runs\n",
    "    print('{}: [{}]'.format(model.__str__(), exp.get_config_log()))\n",
    "    print('| Forward: {:.3f} μs | Backward {:.3f} μs'.format(forward, backward))\n",
    "    return forward, backward\n",
    "\n",
    "def validate_behavior(ModelClass1, ModelClass2, model_weights, exp):\n",
    "    device = exp.device\n",
    "    n_runs = exp.n_runs\n",
    "    embed_size = exp.embed_size\n",
    "    heads = exp.heads\n",
    "    sequence_length = exp.sequence_length\n",
    "    batch_size = exp.batch_size\n",
    "    \n",
    "    values = torch.randn(batch_size, sequence_length, embed_size, device=device)\n",
    "    keys = torch.randn(batch_size, sequence_length, embed_size, device=device)\n",
    "    queries = torch.randn(batch_size, sequence_length, embed_size, device=device)\n",
    "    mask = torch.ones(1,sequence_length, device=device)  # No masking for simplicity\n",
    "\n",
    "    model1 = ModelClass1(embed_size, heads)\n",
    "    model1.reset_parameters(model_weights)\n",
    "    model1 = model1.to(device)\n",
    "    model2 = ModelClass2(embed_size, heads)\n",
    "    model2.reset_parameters(model_weights)\n",
    "    model2 = model2.to(device)\n",
    "\n",
    "    output1 = model1(values, keys, queries, mask)\n",
    "    output2 = model2(values, keys, queries, mask)\n",
    "    \n",
    "    if torch.allclose(output1, output2):\n",
    "        print(f'Model behavior accurate! Sum absolute error: {(output1-output2).abs().sum()}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model behavior accurate! Sum absolute error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Validate model behaviors for Python and C++ implementations\n",
    "\n",
    "model_weights = torch.load('layer_test_weights.pt')\n",
    "exp = ExperimentConfig().set_attention_params(8,10,4,64)\n",
    "validate_behavior(SelfAttentionPython, SelfAttentionExt, model_weights, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 64]\n",
      "| Forward: 652.173 μs | Backward 977.133 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 64]\n",
      "| Forward: 442.050 μs | Backward 1048.186 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 128]\n",
      "| Forward: 611.940 μs | Backward 885.956 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 128]\n",
      "| Forward: 447.146 μs | Backward 1057.088 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 256]\n",
      "| Forward: 711.214 μs | Backward 985.473 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 256]\n",
      "| Forward: 479.777 μs | Backward 1145.667 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 512]\n",
      "| Forward: 639.564 μs | Backward 900.865 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 512]\n",
      "| Forward: 522.705 μs | Backward 1254.668 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 868.574 μs | Backward 1217.871 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 10, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 459.671 μs | Backward 3652.509 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 64]\n",
      "| Forward: 610.791 μs | Backward 920.976 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 64]\n",
      "| Forward: 422.675 μs | Backward 1029.531 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 128]\n",
      "| Forward: 622.891 μs | Backward 880.564 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 128]\n",
      "| Forward: 436.066 μs | Backward 1042.646 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 256]\n",
      "| Forward: 612.123 μs | Backward 888.113 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 256]\n",
      "| Forward: 422.976 μs | Backward 1022.464 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 512]\n",
      "| Forward: 629.465 μs | Backward 882.187 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 512]\n",
      "| Forward: 546.300 μs | Backward 1134.821 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 641.231 μs | Backward 1123.040 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 15, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 584.176 μs | Backward 3969.517 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 64]\n",
      "| Forward: 573.869 μs | Backward 1013.888 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 64]\n",
      "| Forward: 430.061 μs | Backward 1030.306 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 128]\n",
      "| Forward: 611.969 μs | Backward 903.677 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 128]\n",
      "| Forward: 426.371 μs | Backward 1027.461 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 256]\n",
      "| Forward: 625.142 μs | Backward 905.705 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 256]\n",
      "| Forward: 425.983 μs | Backward 1027.602 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 512]\n",
      "| Forward: 644.681 μs | Backward 919.538 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 512]\n",
      "| Forward: 411.435 μs | Backward 1708.765 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 1854.570 μs | Backward 1312.668 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 20, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 3195.185 μs | Backward 2272.251 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 64]\n",
      "| Forward: 603.277 μs | Backward 950.313 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 64]\n",
      "| Forward: 430.787 μs | Backward 1030.948 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 128]\n",
      "| Forward: 614.828 μs | Backward 895.633 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 128]\n",
      "| Forward: 427.077 μs | Backward 1035.702 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 256]\n",
      "| Forward: 628.060 μs | Backward 893.116 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 256]\n",
      "| Forward: 421.581 μs | Backward 1037.592 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 512]\n",
      "| Forward: 731.229 μs | Backward 946.565 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 512]\n",
      "| Forward: 1159.465 μs | Backward 1265.563 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 3226.244 μs | Backward 765.632 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 25, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 4493.799 μs | Backward 1837.877 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 64]\n",
      "| Forward: 749.499 μs | Backward 1092.264 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 64]\n",
      "| Forward: 428.060 μs | Backward 1030.817 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 128]\n",
      "| Forward: 672.580 μs | Backward 945.016 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 128]\n",
      "| Forward: 435.244 μs | Backward 1041.921 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 256]\n",
      "| Forward: 615.584 μs | Backward 903.890 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 256]\n",
      "| Forward: 440.828 μs | Backward 1076.432 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 512]\n",
      "| Forward: 840.156 μs | Backward 1005.989 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 512]\n",
      "| Forward: 1247.444 μs | Backward 1328.531 μs\n",
      "SelfAttention-Python: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 2630.224 μs | Backward 1684.384 μs\n",
      "SelfAttention-Extended: [Batch Size: 16, Seq Length: 30, Heads: 8, Embed Size: 1024]\n",
      "| Forward: 4516.766 μs | Backward 2173.130 μs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run experiments for different attention hyper parameters\n",
    "# Params are in the order: (batch_size, sequence_length, heads, embed_size)\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "exp = ExperimentConfig()\n",
    "batch_sizes = [16]\n",
    "sequence_lengths = [10, 15, 20, 25, 30]\n",
    "heads = [8]\n",
    "embed_sizes = [64, 128, 256, 512, 1024]\n",
    "\n",
    "attention_params = list(itertools.product(batch_sizes, sequence_lengths, heads, embed_sizes))\n",
    "\n",
    "result_matrix = np.zeros((len(attention_params), 8))\n",
    "\n",
    "# Fill in the values row by row\n",
    "for i, attn_params in enumerate(attention_params):\n",
    "    exp.set_attention_params(*attn_params)\n",
    "    fp, bp = run_experiment(exp, SelfAttentionPython)\n",
    "    fe, be = run_experiment(exp, SelfAttentionExt)\n",
    "    result_matrix[i] = [*attn_params, fp, bp, fe, be]\n",
    "\n",
    "# Print the result matrix\n",
    "# print(result_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  16.        ,   10.        ,    8.        ,   64.        ,\n",
       "         652.17297077,  977.13329792,  442.04978943, 1048.18589687],\n",
       "       [  16.        ,   10.        ,    8.        ,  128.        ,\n",
       "         611.93990707,  885.9559536 ,  447.14558125, 1057.08782673],\n",
       "       [  16.        ,   10.        ,    8.        ,  256.        ,\n",
       "         711.21425629,  985.47291756,  479.77700233, 1145.66671848],\n",
       "       [  16.        ,   10.        ,    8.        ,  512.        ,\n",
       "         639.5637989 ,  900.86462498,  522.70460129, 1254.6680212 ],\n",
       "       [  16.        ,   10.        ,    8.        , 1024.        ,\n",
       "         868.57414246, 1217.87090302,  459.67066288, 3652.50890255],\n",
       "       [  16.        ,   15.        ,    8.        ,   64.        ,\n",
       "         610.79080105,  920.97642422,  422.67479897, 1029.53064442],\n",
       "       [  16.        ,   15.        ,    8.        ,  128.        ,\n",
       "         622.89149761,  880.56433201,  436.06581688, 1042.64583588],\n",
       "       [  16.        ,   15.        ,    8.        ,  256.        ,\n",
       "         612.1227026 ,  888.1125927 ,  422.97646999, 1022.46360779],\n",
       "       [  16.        ,   15.        ,    8.        ,  512.        ,\n",
       "         629.46546078,  882.18727112,  546.2998867 , 1134.82093811],\n",
       "       [  16.        ,   15.        ,    8.        , 1024.        ,\n",
       "         641.23051167, 1123.04046154,  584.17644501, 3969.5173502 ],\n",
       "       [  16.        ,   20.        ,    8.        ,   64.        ,\n",
       "         573.86856079, 1013.88776302,  430.06074429, 1030.30586243],\n",
       "       [  16.        ,   20.        ,    8.        ,  128.        ,\n",
       "         611.96937561,  903.67708206,  426.37107372, 1027.46117115],\n",
       "       [  16.        ,   20.        ,    8.        ,  256.        ,\n",
       "         625.14238358,  905.70538044,  425.98340511, 1027.60241032],\n",
       "       [  16.        ,   20.        ,    8.        ,  512.        ,\n",
       "         644.68085766,  919.53773499,  411.43538952, 1708.7651968 ],\n",
       "       [  16.        ,   20.        ,    8.        , 1024.        ,\n",
       "        1854.56955433, 1312.66767979, 3195.18501759, 2272.25103378],\n",
       "       [  16.        ,   25.        ,    8.        ,   64.        ,\n",
       "         603.27734947,  950.31342506,  430.78668118, 1030.94778061],\n",
       "       [  16.        ,   25.        ,    8.        ,  128.        ,\n",
       "         614.82772827,  895.63291073,  427.07664967, 1035.7022047 ],\n",
       "       [  16.        ,   25.        ,    8.        ,  256.        ,\n",
       "         628.05950642,  893.11616421,  421.58071995, 1037.59205341],\n",
       "       [  16.        ,   25.        ,    8.        ,  512.        ,\n",
       "         731.22889996,  946.56503201, 1159.46516991, 1265.56251049],\n",
       "       [  16.        ,   25.        ,    8.        , 1024.        ,\n",
       "        3226.24387741,  765.63227177, 4493.79925728, 1837.87662983],\n",
       "       [  16.        ,   30.        ,    8.        ,   64.        ,\n",
       "         749.4989872 , 1092.26438999,  428.06041241, 1030.81657887],\n",
       "       [  16.        ,   30.        ,    8.        ,  128.        ,\n",
       "         672.57962227,  945.01562119,  435.24439335, 1041.92073345],\n",
       "       [  16.        ,   30.        ,    8.        ,  256.        ,\n",
       "         615.5837059 ,  903.89008522,  440.82796574, 1076.43196583],\n",
       "       [  16.        ,   30.        ,    8.        ,  512.        ,\n",
       "         840.15550613, 1005.98943233, 1247.44384289, 1328.53109837],\n",
       "       [  16.        ,   30.        ,    8.        , 1024.        ,\n",
       "        2630.22401333, 1684.3839407 , 4516.76566601, 2173.13039303]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SelfAttentionCustom | Forward: 3.465 s | Backward 6.476 s\n",
    "# SelfAttention | Forward: 4.863 s | Backward 5.056 s\n",
    "\n",
    "# CPU runs\n",
    "# CPU | SelfAttentionCustom | Forward: 358.924 μs | Backward 577.587 μs\n",
    "# CPU | SelfAttention | Forward: 492.750 μs | Backward 466.254 μs\n",
    "\n",
    "# CUDA runs\n",
    "# GPU | SelfAttentionCustom | Forward: 619.070 μs | Backward 1141.094 μs\n",
    "# GPU | SelfAttention | Forward: 951.625 μs | Backward 1176.297 μs\n",
    "\n",
    "# GPU | SelfAttentionCustom | Forward: 789.302 μs | Backward 1287.836 μs\n",
    "# GPU | SelfAttention | Forward: 1012.441 μs | Backward 1103.148 μs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
